# -*- coding: utf-8 -*-
"""perceptron modell.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uY6-SJ1hBKIP4Pmt16MG7H0Se54Xpgna
"""

from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt
import numpy as np

X, Y = make_blobs(n_features=2, centers=2, n_samples=1000, random_state=12)
print(type(X[:,0]))

plt.figure(figsize=(6, 6))
plt.scatter(X[:, 0], X[:, 1], c=Y)
plt.title('Ground truth', fontsize=18)
plt.show()

w = np.random.rand(3, 1)
print(w)

X_bias = np.ones((X.shape[0], 3))
print(X_bias)
X_bias[:, 1:3] = X
#w=w+(exp -pred)*x
print(X_bias)

def activation_func(z):
    if z >= 1:
        return 1
    else:
        return 0

for _ in range(100):
    for i in range(X.shape[0]):
        y = activation_func(w.transpose().dot(X_bias[i, :]))

        w = w + ((Y[i] - y) * X_bias[i, :]).reshape(w.shape[0], 1)

print("Final Weights:", w)

result_class = [activation_func(w.transpose().dot(x)) for x in X_bias]

w1 = w / np.sqrt(w.transpose().dot(w))
print(w1)

plt.figure(figsize=(6, 6))
plt.scatter(X[:, 0], X[:, 1], c=result_class)

xx = np.linspace(-10, 1)
yy = -w1[1] * xx / w1[2] - w1[0] / w1[2]
a=-w1[0] / w1[1]


plt.plot(xx, yy, lw=3, c='red')
plt.title('Perceptron classification with decision boundary')
plt.show()

from sklearn.metrics import confusion_matrix
confusion_matrix(Y, result_class)

w_p = np.random.rand(3,1)
print(w_p)

x1=np.array([[0,0],[0,1],[1,0],[1,1]])
x1

y1=np.array([0,1,1,1])
x_train_binarized1=x1
y_train1=y1
x1_bias=np.ones((x1.shape[0],3))
print(x1_bias)
x1_bias[:, 1:3]=x1
print(x1_bias)

x1_bias[:, 1:3]=x1
print(x1_bias)

#import numpy as np
#import matplotlib.pyplot as plt
"""
x1=np.array([[0,0],[0,1],[1,0],[1,1]])
y1=np.array([0,1,1,1])
x1_bias=np.ones((x1.shape[0],3))
x1_bias[:, 1:3]=x1
print(x1_bias)"""

#w_p = np.random.rand(3,1)

def activation_func(z):
    if z >= 0:
        return 1
    else:
        return 0

for _ in range(100):
    for i in range(x1_bias.shape[0]):
        y_pred = activation_func(w_p.transpose().dot(x1_bias[i, :]))

        w_p = w_p + ((y1[i] - y_pred) * x1_bias[i, :]).reshape(w_p.shape[0], 1)


plt.figure(figsize=(6, 6))
plt.scatter(x1[:, 0], x1[:, 1], c=y1)
plt.title('Ground truth', fontsize=18)
plt.show()

import numpy as np
import matplotlib.pyplot as plt
"""
x1=np.array([[0,0],[0,1],[1,0],[1,1]])
y1=np.array([0,1,1,1])
x1_bias=np.ones((x1.shape[0],3))
x1_bias[:, 1:3]=x1

def activation_func(z):
    if z >= 0:
        return 1
    else:
        return 0"""

# predicting the class of the datapoints
result_class = [activation_func((w_p.transpose().dot(x))) for x in x1_bias]

# visualize the decision boundary and the predicted class labels

# convert to unit vector
w2 = w_p/np.sqrt((w_p.transpose().dot(w_p)))
print(w2)

# Visualize results
plt.figure(figsize = (6, 6))
plt.scatter(x1[:, 0], x1[:, 1], c = result_class)
xx = np.linspace(-0.5,1.5)
yy = -(w2[0]/w2[2]) - (w2[1]/w2[2])*xx
plt.plot(xx, yy, lw = 3, c = 'red')
plt.title('Perceptron classification with decision boundary')
plt.show()

w_p = np.random.rand(3,1)
x1=np.array([[0,0],[0,1],[1,0],[1,1]])
y1=np.array([0,1,1,1])
x_train_binarized1=x1
y_train1=y1
x1_bias=np.ones((x1.shape[0],3))
x1_bias[:, 1:3]=x1
print(x1_bias)

def activation_func(z):
    if z >= 0:
        return 1
    else:
        return 0

print(x1_bias)

for _ in range(100):
    for i in range(x1_bias.shape[0]):
        y_pred = activation_func((w_p.transpose().dot(x1_bias[i,:].T)))
        # update weights
        w_p = w_p + ((y1[i] - y_pred) * x1_bias[i,:].reshape(w_p.shape[0], 1))

plt.figure(figsize = (6, 6))
plt.scatter(x1[:, 0], x1[:, 1], c = y1)
plt.title('Ground truth', fontsize = 18)
plt.show()

"""
import numpy as np
import matplotlib.pyplot as plt

x1=np.array([[0,0],[0,1],[1,0],[1,1]])
y1=np.array([0,1,1,1])
x1_bias=np.ones((x1.shape[0],3))
x1_bias[:, 1:3]=x1

def activation_func(z):
    if z >= 0:
        return 1
    else:
        return 0  """

# predicting the class of the datapoints
result_class = [activation_func((w_p.transpose().dot(x))) for x in x1_bias]

# visualize the decision boundary and the predicted class labels
w2 = w_p/np.sqrt((w_p.transpose().dot(w_p)))
print(w2)

# Visualize results
plt.figure(figsize = (6, 6))
plt.scatter(x1[:, 0], x1[:, 1], c = result_class)
xx = np.linspace(-0.5,1.5)
yy = -(w2[0]/w2[2]) - (w2[1]/w2[2])*xx
plt.plot(xx, yy, lw = 3, c = 'red')
plt.title('Perceptron classification with decision boundary')
plt.show()

import numpy as np

def perceptron(weights, inputs, bias):
    model = np.add(np.dot(inputs, weights), bias)
    logit = activation_function(model, type="sigmoid")
    return np.round(logit)

def activation_function(model, type="sigmoid"):
    return {
        "sigmoid": 1 / (1 + np.exp(-model))
    }[type]

def compute(data, logic_gate, weights, bias):
    weights = np.array(weights)
    output = np.array([ perceptron(weights, datum, bias) for datum in data ])
    return output

# Function from GitHub repo
def print_template(dataset, name, data):
    print("Logic Function: {}".format(name.upper()))
    print("x1\tx2\tY_hat")
    for inp, datas in zip(dataset, data):
        print("{}\t{}\t{}".format(*datas))

def main():
    dataset = np.array([
        [0, 0, 1],
        [0, 1, 1],
        [1, 0, 1],
        [1, 1, 1]
    ])

    gates = {
        "and": compute(dataset, "and", [1, 1, 1], -2),
        "or": compute(dataset, "or", [1, 1, 1], -1),
        "nand": compute(dataset, "nand", [-1, -1, -1], 3),
        "nor": compute(dataset, "nor", [-1, -1, -1], 2),
        "xor": compute(dataset, "xor", [1, 1, 1], -1)
    }

    for name, data in gates.items():
        print_template(dataset, name, dataset)

if __name__ == "__main__":
    main()